{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./utils/LogoL2RPN.jpg\", width=150, ALIGN=\"left\", border=10>\n",
    "<h1>L2RPN Starting Kit </h1> \n",
    "\n",
    "\n",
    "ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED \"AS-IS\". The CDS, CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, AND THE WARRANTY OF NON-INFRIGEMENT OF ANY THIRD PARTY'S INTELLECTUAL PROPERTY RIGHTS. IN NO EVENT SHALL AUTHORS AND ORGANIZERS BE LIABLE FOR ANY SPECIAL, \n",
    "INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF SOFTWARE, DOCUMENTS, MATERIALS, PUBLICATIONS, OR INFORMATION MADE AVAILABLE FOR THE CHALLENGE. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The goal of this challenge is to use Reinforcement Learning approaches for managing a powergrid. The Reinforcement Learning agents will have to automate the control of the powergrid. We use the power network simulator <a href=\"https://github.com/rte-france/Grid2Op\">Grid2Op</a>. It is a simulator that is able to emulate a powergrid of any size and its electrical properties depending on the temporal injections (electricity production and consumption) at each time step.\n",
    "\n",
    "## Goal of this notebook\n",
    "This notebook will briefly describe the how this competition will run. \n",
    "\n",
    "We apologize in advance for its length.\n",
    "\n",
    "## References and credits:\n",
    "\n",
    "The creator of Grid2Op was Benjamin Donnot. The competition was designed by Antoine Marot, Benjamin Donnot, Karim Chaouache, Adrian Kelly, Ramij Raja Hossain, Jochen Cremer, Qiuhua Huang. Additional thanks to Pratik Kulkarni, Dexter Lewis and Matt Petree from EPRI for helping with various aspects of the competition setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"nutshell\"></a>\n",
    "<h2 style=\"font-family:'Verdana',sans-serif; color:#1D7874;\">L2RPN ICAPS Track in a nutshell</h2>\n",
    "    \n",
    "<p style=\"font-family:'Verdana','sans-serif'; color:#393D3F; text-align:justify; font-size:14px;\">\n",
    "In this L2RPN competition you are asked to submit an agent that operates a powergrid with trust. More specifically, 1) your agent need to survive (avoid blackout) for the longest number of time steps possible and 2) can alert the system operators at the moment of crisis by raising an alarm.\n",
    "<br>    \n",
    "  You have to submit the code of your agent as it will be evaluated on the cloud. All participants are evaluated with the same input and the same environment having the same seed. The competition is split into three phase:\n",
    "</p>\n",
    "\n",
    "| Phase      |  Start     |  End       | Description      | Max submissions per day | Max submissions total |\n",
    "|:-----------|:-----------|:-----------|:-----------------|:-------:|:--------:|\n",
    "| warmup     | 25/06/2021 | 26/07/2021 |  The warmup phase is designed to let everyone experiment with the proposed problem, create and submit early agents. During warmup, participants may find bugs, unexpected behaviors and other errors in the track setup. In that case, the organizers are commited to assess, correct and update the competition software and datasets based on the community feedback.   | 20 | 1000\n",
    "| validation | 27/07/2021 | 22/09/2021 |  The validation phase is the main competition phase. Participants are allowed to submit their code up to 20 times a day within the limit of 1,000 total submissions during the phase.  | 20 | 1000 |\n",
    "| test       | 22/09/2021 | 24/09/2021 |  In the final phase, the **last** solution from the validation phase of each participant is tested on a new dataset and ranked by score. Winners will be announced after all submissions have been tested and verified. | 1 | 1 |\n",
    "\n",
    "<p style=\"font-family:'Verdana','sans-serif'; color:#393D3F; text-align:justify; font-size:14px;\">\n",
    "In this track, your agent will be evaluated in a presence of an *Opponent* that can take malicious (adversarial) attack to disturb you from achieving your goal.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT: Description of the competition rules\n",
    "\n",
    "***DISCLAIMER*** This part is here to describe how the competition will take place and to bring some description on its rules. Official rules of the competition are distributed in the competition description (on the codalab platform) under the *Terms and Conditions*. Also we remind that this competition is subject to the [general ChaLearn contest rules](http://www.causality.inf.ethz.ch/GeneralChalearnContestRuleTerms.html). It is not possible to use the information given on this notebook to contradict the rules given there. This section is **not part of the official rules**.\n",
    "\n",
    "The competition will take place in three distinct phases, each discribed in one subsection.\n",
    "\n",
    "## The warmup phase\n",
    "This is the first phase of the competition. A warm up phase takes place. During this warmup phase, participants are allowed to make submission on a private dataset (called the *warmup dataset*). This dataset is not included in the training data in any way. It has however the exact same properties: the powergrid has the name number of powerlines, of loads, of generators. They are at the same place, labelled with the same names, have the same physical properties etc.\n",
    "\n",
    "During this phase, participants are encourage to ask question and give feedbacks to the organizers through mails or in our discord server (https://discord.gg/cYsYrPT). If **some additional packages** find useful to the participants, they are welcome to ask if they can be included when running the submission on codalab for the next pahses. If you spot any \n",
    "bug, inconsistent behavior or lack of documentation you are more than welcome to report them.\n",
    "\n",
    "If needed, at the end of this phase, organizers can fix the version of grid2op or adapt any of the rules of competition based on community feedback.\n",
    "\n",
    "## The validation phase\n",
    "This represent the real phase of the competition. During this phase, participants will be asked to submit their agents on a different dataset than both the training set and the warmup set. This set will also be private. It has been selected with specific rules from a list of scenarios generated with the same statistical distribution than the training set. We give the guarantee that this method of selection will be the same between the validation phase and the test phase (see below).\n",
    "\n",
    "During this phase (except if some critical bug is discovered or in case of data leakage or in any other critical issue) the version of grid2op used to rank the agent will NOT change. It should be the same one as in the test phase.\n",
    "\n",
    "Participants feedback is still really valuable and we will continue answer to your question on our discord server (https://discord.gg/cYsYrPT) of course. \n",
    "\n",
    "## Test phase\n",
    "Only the score of your agent on this specific phase will be use for the leaderboard of the competition.\n",
    "\n",
    "**You will not be able to submit anything to this phase**. This phase will last only few hours. The last valid submission on  each participant will be ***automatically*** \"migrated\" to this phase. Only the last agent on this phase will give be used for the final score.\n",
    "\n",
    "\n",
    "## Note on the \"Migration\"\n",
    "Migration is the process with which your agent are automatically ranked in the test phase.\n",
    "\n",
    "This happens automatically, you have nothing to do. You cannot do anything either.\n",
    "\n",
    "Multiple migration can happen for the same participant. For example, if you submit 20 valid agents during the entire validation phase, you will get at least 1 agent migrated and at most 20 agent migrated (exact number depends on the times at which you submit). This entails that some participants will have multiple submission on the final leaderboard displayed on codalab at the end of the competition.\n",
    "\n",
    "We also have a limited number of machines to process your submission. This is why the migration process might take some time. This entails that there might be some delay between the end of the competition and your appearance on the leaderboard of the test phase (this is why the test phase lasts a few hours!). And this also explains why the \"submission date\" in the leaderboard will actually be greater than the last possible dates for which you were able to do submission on the validation phase.\n",
    "\n",
    "For example this was the leaderboard displayed at the end of the WCCI competition:\n",
    "\n",
    "![](./utils/img/LB_example.png)\n",
    "\n",
    "Note how some participants gets multiple submissions. But by design only the ***last one*** will be use to make the final leaderboard.\n",
    "\n",
    "## Final leaderboard\n",
    "\n",
    "Last important note we want to make is the evaluation of the final leaderboard. We included in the rules of the competition some rules that cannot be checked informatically (or that we don't want to check informatically and automatically at each submission). This entails that the final leaderboard on codalab is NOT the final leaderboard of the competition. Final leaderboard of the competition will be updated in our discord and on our website.\n",
    "\n",
    "We want to emphize two importants rules\n",
    "\n",
    "### Double accounts are forbidden\n",
    "We have an explicit rule: 1 team = 1 account.\n",
    "\n",
    "If we notice that the participants of the same team appears more than once in the final leaderboard, we remove all associated entries. We won't keep the best one, we won't pick one at random, we will simply removed all double \"double submissions\" from this competition.\n",
    "\n",
    "If you want to compete as a team, the best way is to create a common mail adress, one mail address all team members will have access to.\n",
    "\n",
    "We allow participants to form team during the \"validation phase\" of the competition and only at least **2 weeks before the beginning of the test phase**. If that is the case, please send the organiser an email (or a message on discord) with the name (on codalab) and mail addresses of each team member. We will proceed to the erase of all team members submission in the test set to avoid duplicate at that time. \n",
    "\n",
    "Note that in this case, if any team members submit another agent with his account after the team has been created and validated by one of the organizer, it will break the \"double accounts are forbidden\" rules this is thus forbidden (errors can happen, if you realise you make a mistake come forward and we'll fix it for you for sure).\n",
    "\n",
    "### Choose your league \"new comers\" & \"anyone\"\n",
    "One has to choose **2 weeks before the beginning of the test phase** in which league he wants to eventually compete for the prizes. You cannot cumulate prizes between the two leagues.\n",
    "\n",
    "\n",
    "### Submission are limited in size\n",
    "\n",
    "We don't check the size of your model in the codalab platform. Know that submissions are limited to 500MB in size (real size might vary and this notebook will not be necessarily updated, the real size is written in the ***Terms and Conditions*** page of the official competition). This means that if the zip file you send to codalab is more than 500MB, it will be removed from the leadeboard.\n",
    "\n",
    "### Prizes and open sourcing of submission\n",
    "We recall that in section *8 Prizes and Awards* section *(a) Prizes* of the [general ChaLearn contest rules](http://www.causality.inf.ethz.ch/GeneralChalearnContestRuleTerms.html) stipulates that, in order to receive your price, you ***must*** open source your code (this is also reminded in the  ***Terms and Conditions***) [*disclaimer: this wording might not be legally correct in all countries and in all domains. We recall that this paragraph is NOT part of the rules. Please refer to the header ot this section for more information.*].\n",
    "\n",
    "For this specific competition this means that you should, in order to receive your price (in case you are elligible to such price), follow the template baseline of the [l2rpn-baselines](https://github.com/rte-france/l2rpn-baselines) github repository available at this page [CONTRIBUTE](https://github.com/rte-france/l2rpn-baselines/blob/master/CONTRIBUTE.md).\n",
    "\n",
    "What does it mean? It means that anyone that wants it (under the licensing of your chosing, see more information on the chalearn contest rules and on the l2rpn-baselines github) should be able to run your *trained agent* on the very same competition. Your trained agent will also be somehow included in the l2rpn-baselines python package.\n",
    "\n",
    "What it does not mean? It does not mean that your *training code*. Of course, this is a competition toward an open source project aiming at as much transparency as possible. But we understand that for some person (academics who's paper are under review, or industrial method) it is not possible to ask to disclose the training methodology. This is the main reason why we allow you not to share the code of the training of your agent in case you are eligible to some prices.\n",
    "\n",
    "**NB** These competitions aim at closing the gap between industrial real time grid operation and planning and the research community in \"Sequential Decision Making\" to help develop further new kind of industrial or academic partnership.\n",
    "\n",
    "RTE (main organizer), nor any of the competition sponsors, will NEVER use your algorithm without your consent. Your code is subject to the international laws on copy right and (if applicable) to the license (open source or not) you released it under. We will never disclose your code to anyone (except to a group of experts from RTE and other relevant affiliation to look at it to make sure it is compliant with the rules of the competition). \n",
    "\n",
    "\n",
    "# Important note on the data\n",
    "\n",
    "Power systems are rather complex systems. It is not easy to generate \"realistic\" data in an automatic manner. To make this competition happen we had two choices:\n",
    "\n",
    "- Providing each participant a data generator so that each of you can generate as much data as you want, but without being sure these datasets were realistic\n",
    "- Providing each participants with a limited number of data (that we called scenarios) that we carefully checked in order to make sure they were \"realistic\" or rather \"as realistic as the data your agents will be tested on\".\n",
    "\n",
    "We decided to go for the second option for multiple reasons. The first reason is practical: it is to reduce the entry cost in these competitions. Indeed having to use an external tool to generate the data would have been a barrier. Typing a command like `grid2op.make(xxx)` is by far easier. The main reason also is that the generation process can be quite complex and it is easy to have irrealistic datasets. \n",
    "\n",
    "Your agent can be trained on a fix number of *scenarios* that we provide (we call that the *training set*), this is the data you will retrieve with the command in the next section. And will be tested on a fix number of scenarios that we kept hidden (one set of scenarios for the *warm up phase* one different set of scenarios for the *validation phase* and one for the *test phase*).\n",
    "\n",
    "You can compare our concept of a *scenario* with the concept of a *level* in certain video game, Prince of Persia, Rayman, Mario etc. In that case, for each track, your agent will play always the same game (say [Super_Mario_Bros](https://en.wikipedia.org/wiki/Super_Mario#Super_Mario_Bros.)). Your agent can train on a fix set of levels (the *training levels*) and will be tested on different (by similar and coming from the same statistical distribution) levels on our servers (when you submit your agent on codalab, see next notebooks).\n",
    "\n",
    "For each of the neurips track we provide two different kind of *training datasets*. One \"*small*\" and one \"*large*\" dataset. The small one is a subset of the first element of the large one. We don't recommend you to use the \"large\" dataset at first, why ?\n",
    "\n",
    "### Data for L2RPn with Trust\n",
    "For this competition, the \"*small*\" dataset contains 50 years of data. Each years counts 12 months (*yes yes even in power system the years counts 12 months;-)*) and each month have 4 weeks (*wait what ??? Yes for this competition month of March will have 28 days. I told you in power system we didn't like to do things like everyone else*) each of 7 days counting exactly 24 hours (no time change in these simulated dataset). We decided that a \"step\" reprenseted 5 mins for this competition.\n",
    "\n",
    "This means that for the *small* dataset you have at your disposal `50 years * 12 months * 4 weeks * 7 days * 24 hours * (60/5) steps = 4.838.400` different input data. This is a pretty large number of powergrid states, trust me. If predicting an action with a neural network takes like 10 ms (which would be a super fast model) you will need (without training!, just to predict the best action in all these states) approximately 13 consecutive hours. This dataset weights 800~900MB\n",
    "\n",
    "The large dataset is made of at least 200 years but the number of scenarios per month can vary. This is really large and we don't recommend you to download this *large* dataset unless you feel pretty confident that your model is lacking training data. This dataset weights 4.5-5GB.\n",
    "\n",
    "**NB** actually, the *scenarios* are not deterministic, they are stochastics (due to maintenance) and adversarial (with the opponent). In our opinion, even using the *small* dataset is enough for the vast majority of models to perform well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
