{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a submission on Codalab\n",
    "\n",
    "## Unit testing\n",
    "\n",
    "It is **important that you test your submission files before submitting them**. All you have to do to make a submission is create or modify the `make_agent` and `reward` variables exported in the `__init__.py` script as seen earlier. Then make sure that everything works fine with the following test script. This is the actual program that will run on the server to test your submission.\n",
    "\n",
    "\n",
    "**NB** The training dataset provided in this package is different than the private datasets used on codalab (see the section about the data in the [previous notebook](3_Rules_Data_Score_Agent.ipynb)). It is normal that your agent will have a very different score on codalab than here locally.\n",
    "\n",
    "Also, the hidden dataset on which your agent will be evaluated has been carefully picked so as to offer different levels of difficulty, but the provided training dataset has NOT. For that reason, the scores will likely not be similar, even though these two datasets originate from the same statistical distribution.\n",
    "\n",
    "Also, on codalab, your agent might timeout if it takes too much time to run. Powergrid need to be operated in real time with relatively hard time constraints (you cannot take more than 5 minutes to take an action each 5 minutes).\n",
    "\n",
    "The next two cells will test that everything is working correctly and that the submission can be sent. That process will run the agent on a small portion of the training dataset. **It will also generate the zip file that you will have to upload to Codalab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO: Basic check and creation of the zip file for the folder example_submissions/submission\n",
      "\n",
      "INFO: No custom reward for the assessment of your agent will be used. If you want to use a custom reward when your agent is evaluated, make sure to export  \"reward\", which should be a class inheriting from grid2op.BaseReward in your module (done in __init__.py).\n",
      "INFO: No custom other_rewards for the assessment of your agent will be used. If you want to get information about other rewards when your agent is evaluated, make sure to export  \"other_rewards\" dictionnary in your module (you can do it in the __init__.py file)\n",
      "Your submission appear to be valid. For more test, we encourage you to run the appropriate notebook to do these unit testing.\n",
      "The zip file \"/home/horacio/git/L2RPN/L2RPN_icaps2021_starting_kit/submission_21-07-04-18-28.zip\" has been created with your submission in it.\n",
      "\n",
      "INFO: Checking the zip file can be unzipped.\n",
      "\n",
      "\n",
      "INFO: Checking content is valid\n",
      "\n",
      "\n",
      "INFO: metadata found.\n",
      "\n",
      "\n",
      "INFO: This might take a while..\n",
      "It will evaluate your agent on a whole lot of scenarios\n",
      "(24 scenarios, with similar number of timesteps than the private datasets on codalab)\n",
      "\n",
      "--------\n",
      "\n",
      "ERROR: Your agent could not be run. \n",
      "It will probably fail on codalab.\n",
      "Here is the information we have:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You can run \n",
      "\"/usr/bin/python3 /home/horacio/git/L2RPN/L2RPN_icaps2021_starting_kit/utils/ingestion_program_local//ingestion.py --input_path /home/horacio/git/L2RPN/L2RPN_icaps2021_starting_kit/input_data_local/ --output_path utils/last_submission_results/res --program_path /home/horacio/git/L2RPN/L2RPN_icaps2021_starting_kit/utils/ingestion_program_local/ --submission_path /tmp/tmpsfhpipyf --key_score grid_operation_cost --config_in /home/horacio/git/L2RPN/L2RPN_icaps2021_starting_kit/utils/ingestion_program_local//config_local.json --nb_episode 24 --gif_episode apr_17_1 --gif_start 275 --gif_end 325\"\n",
      " for more debug information\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "INVALID SUBMISSION",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-65ae714f3513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'example_submissions/submission'\u001b[0m \u001b[0;31m# your submission directory, can have any names here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/L2RPN/L2RPN_icaps2021_starting_kit/check_your_submission.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model_dir)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mevaluate_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/L2RPN/L2RPN_icaps2021_starting_kit/check_your_submission.py\u001b[0m in \u001b[0;36mevaluate_submission\u001b[0;34m(model_dir, input_data_dir, ingestion_program_dir, scoring_program_dir, agent_name)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can run \\n\\\"{}\\\"\\n for more debug information\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mli_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INVALID SUBMISSION\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO_RUN_SUCCESS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: INVALID SUBMISSION"
     ]
    }
   ],
   "source": [
    "from check_your_submission import main as test_submission\n",
    "\n",
    "model_dir = 'example_submissions/submission' # your submission directory, can have any names here\n",
    "test_submission(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your submission ran smoothly, you can even see summarized results in this notebook in the next two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"utils/last_submission_results/results.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from IPython.display import Image\n",
    "my_gif = glob.glob(\"utils/last_submission_results/*.gif\")\n",
    "img = None\n",
    "if len(my_gif):\n",
    "    my_gif = my_gif[0]\n",
    "    img = Image(filename=my_gif)\n",
    "    display(img)\n",
    "else:\n",
    "    print(\"No gif were generated, your agent probably fails before.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same kind of information are available on codalab! You can retrieve these informations by looking at the \"*Download output from scoring step*\" button once your submission has finished (more information on the section **Other information about your submission** bellow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the previous test fails\n",
    "\n",
    "If the previous test fails, check the notebook [5_DebugYourSubmission](5_DebugYourSubmission.ipynb) to try to debug your submission.\n",
    "\n",
    "If the previous test fails, check that your folder is named **\"submission\"** and read again the notebook [3_Develop_And_RunLocally_An_agent](3_Develop_And_RunLocally_An_agent.ipynb) carefully. In particular, check that your code is well structured, as explained in the previous notebook, and that the `__init__.py` script indeed defines a `make_agent` function, and optionally a `reward` class. Also check that your `make_agent` function and your agent have the correct signatures, as we explained.\n",
    "\n",
    "If you still need help, do not hesitate to reach out to us on the dedicated discord server here : https://discord.gg/cYsYrPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to Codalab\n",
    "\n",
    "The previous test generated a zip file for you. Its location was printed in the cell output so you can find it.\n",
    "\n",
    "If the previous test ran correctly, all you have to do is upload it on codalab.\n",
    "\n",
    "To that end, you first need to go into the competition home page, and click on the \"participate\" section:\n",
    "\n",
    "![](utils/img/Codalab_submit.png)\n",
    "\n",
    "Then click on the \"submit / view results\":\n",
    "![](utils/img/Codalab_view_res.png)\n",
    "\n",
    "Then you can click on the \"submit\" button:\n",
    "![](utils/img/Codalab_submit2.png)\n",
    "\n",
    "A window will appear that ask you what you want to select. Select the proper submission and clik on it.\n",
    "![](utils/img/Codalab_submit3.png)\n",
    "\n",
    "Then you will see something like this:\n",
    "![](utils/img/Codalab_submit4.png)\n",
    "\n",
    "\n",
    "At that time it means that your submission is being evaluated on our servers. As you know, the code of your agent will be evaluated on our platform. This might take some times (around 30 mins if everything goes well). \n",
    "\n",
    "**NB** our servers are limited in number. In case lots of people are making submission at the same time, the execution time of your code might be longer than these 30 minutes (for example if we have 10 workers to compute the submissions, the first 10 submitted will be handled without delay - *maybe a few minutes, the time to upload the data etc.* -  but the 11th one will only be handled when the first one -*among the first 10*- is over)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other information about your submission\n",
    "The score is not the only things that matters !\n",
    "\n",
    "You have other details about your submission if you look at the \"download output from scoring step\" (you will get in particular the html and the gif for your submission!) and the \"download output from prediction step\" (where you can get relevant information for each hidden scenario and how well your agent performed on each of them, especially if you modified the default \"reward\" and \"other_rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}