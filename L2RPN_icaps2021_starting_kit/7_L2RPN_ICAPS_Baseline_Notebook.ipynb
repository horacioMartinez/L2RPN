{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRFdYVrQ8Grv"
   },
   "source": [
    "# **L2RPN-ICAPS Example Run of RL Agents**\n",
    "\n",
    "This short tutorial notebook provides a quick guidance for installing and testing some Reinforcement Learning (RL) algorithms with Grid2Op framework. The RL algorithm used in **Section-I** is taken from [l2rpn_baselines](https://github.com/rte-france/l2rpn-baselines/tree/master/l2rpn_baselines), and that used in **Section-II** is taken from [Ray-RLlib](https://docs.ray.io/en/master/rllib.html). \n",
    "\n",
    "**A quick walkthrough:**\n",
    "- Install Grid2op and l2rpn_baselines using pip command.\n",
    "- Sample codes of DeepQSimple, DuelQSimple, DuelQLeapNet, DoubleDuelingDQN, DoubleDuelingRDQN are available in l2rpn_baselines, for brevity only the usage of DeepQSimple is shown in **Section-I**.\n",
    "- Please note these codes are just used to show the implementation. The performnaces are not tuned for the given codes. The action space, observation space and neural network architecture are chosen randomly.\n",
    "- \"l2rpn_neurips_2020_track1_small\" is used as the environment for this example.\n",
    "- Please note, to use expert_agent (can be found in l2rpn_baselines), one need to install [ExpertOp4Grid](https://expertop4grid.readthedocs.io/en/latest/).\n",
    "\n",
    "- In **Section-II**, install RLlib. The DQN algorithms from RLlib are implemented as examples. Here, also the performances are not tuned. Check [training API](https://docs.ray.io/en/master/rllib-training.html) for RLlib algoritms.\n",
    "- Please note, to use grid2op environment with RLlib, there is a need to tighten the gap between grid2op and OpenAI Gym environments. Hence, The observation space and action space are made compatible with gym enviorment. To learn more on this, please check [grid2op.gym_compat](https://grid2op.readthedocs.io/en/latest/gym.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lulCCvnA_XuU"
   },
   "source": [
    "# **Section-I (RL Algorithms from l2rpn_baselines)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWnRs9FU_TMq"
   },
   "outputs": [],
   "source": [
    "#!pip3 install grid2op  # for use with google colab (grid2Op is not installed by default)\n",
    "#!pip3 install l2rpn_baselines.   # for use with google colab (l2rpn_baselines is not installed by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFvZBP2__Za0"
   },
   "source": [
    "# **Sample Code for Training // Algorithm: DeepQSimple**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOr4TWRiJLt9",
    "outputId": "e13438f4-6d78-4389-ffbb-adc17c08efe3"
   },
   "outputs": [],
   "source": [
    "import grid2op\n",
    "from grid2op.Reward import L2RPNReward\n",
    "from l2rpn_baselines.utils import TrainingParam, NNParam\n",
    "from l2rpn_baselines.DeepQSimple import train\n",
    "# define the environment\n",
    "env = grid2op.make(\"l2rpn_icaps_2021_small\",\n",
    "                    reward_class=L2RPNReward)\n",
    "# use the default training parameters\n",
    "tp = TrainingParam()\n",
    "# this will be the list of what part of the observation I want to keep\n",
    "# more information on https://grid2op.readthedocs.io/en/latest/observation.html#main-observation-attributes\n",
    "li_attr_obs_X = [\"day_of_week\", \"hour_of_day\", \"minute_of_hour\", \"prod_p\", \"prod_v\", \"load_p\", \"load_q\",\n",
    "                  \"actual_dispatch\", \"target_dispatch\", \"topo_vect\", \"time_before_cooldown_line\",\n",
    "                  \"time_before_cooldown_sub\", \"rho\", \"timestep_overflow\", \"line_status\"]\n",
    "#li_attr_obs_X = [\"day_of_week\", \"hour_of_day\", \"minute_of_hour\", \"prod_p\", \"prod_v\", \"load_p\", \"load_q\",\n",
    "#\"actual_dispatch\", \"target_dispatch\", \"topo_vect\", \"time_before_cooldown_line\",\n",
    "#\"time_before_cooldown_sub\", \"rho\", \"timestep_overflow\", \"line_status\"]\n",
    "# neural network architecture\n",
    "observation_size = NNParam.get_obs_size(env, li_attr_obs_X)\n",
    "sizes = [800, 800, 800, 494, 494, 494]  # sizes of each hidden layers\n",
    "kwargs_archi = {'observation_size': observation_size,\n",
    "                'sizes': sizes,\n",
    "                'activs': [\"relu\" for _ in sizes],  # all relu activation function\n",
    "                \"list_attr_obs\": li_attr_obs_X}\n",
    "# select some part of the action\n",
    "# more information at https://grid2op.readthedocs.io/en/latest/converter.html#grid2op.Converter.IdToAct.init_converter\n",
    "kwargs_converters = {\"all_actions\": None,\n",
    "                      \"set_line_status\": False,\n",
    "                      \"change_bus_vect\": True,\n",
    "                      \"set_topo_vect\": False\n",
    "                      }\n",
    "# define the name of the model\n",
    "nm_ = \"AnneOnymous\"\n",
    "try:\n",
    "    train(env,\n",
    "          name=nm_,\n",
    "          iterations=100, # Change the no. of interations\n",
    "          save_path=None,\n",
    "          load_path=None,\n",
    "          logs_dir=None,\n",
    "          training_param=tp,\n",
    "          kwargs_converters=kwargs_converters,\n",
    "          kwargs_archi=kwargs_archi)\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcBXIxMd-2co"
   },
   "source": [
    "# **Section-II** **(RL Algorithms from Ray-RLlib)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1gTyQdlTzQ1"
   },
   "source": [
    "**Installation of RlLib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "!$sys.executable -m pip install 'ray[rllib]' # Install RLLib\n",
    "!pip install tensorflow==2.2.0#make sure to have tensorflow<=2.2.0 to work fine for now with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lH_ZOq_AyNG"
   },
   "source": [
    "# **RLlib code for DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aITmevOv1zz"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import ray\n",
    "import gym\n",
    "import numpy as np\n",
    "from ray.tune.logger import pretty_print\n",
    "import shutil\n",
    "import os\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        import grid2op\n",
    "        from grid2op.gym_compat import GymEnv\n",
    "        from grid2op.gym_compat import ScalerAttrConverter, ContinuousToDiscreteConverter, MultiToTupleConverter\n",
    "        from grid2op.gym_compat import DiscreteActSpace\n",
    "        from grid2op.Reward import GameplayReward, L2RPNReward\n",
    "\n",
    "\n",
    "        # 1. create the grid2op environment\n",
    "        if not \"env_name\" in env_config:\n",
    "            raise RuntimeError(\"The configuration for RLLIB should provide the env name\")\n",
    "        nm_env = env_config[\"env_name\"]\n",
    "        del env_config[\"env_name\"]\n",
    "        self.env_glop = grid2op.make(nm_env, **env_config,reward_class=L2RPNReward)\n",
    "\n",
    "        # 2. create the gym environment\n",
    "        self.env_gym = GymEnv(self.env_glop)\n",
    "        obs_gym = self.env_gym.reset()\n",
    "\n",
    "        # 3. (optional) customize it (see section above for more information)\n",
    "        ## customize action space\n",
    "        self.env_gym.action_space = DiscreteActSpace(self.env_glop.action_space,\n",
    "                                                     attr_to_keep=['change_line_status'])\n",
    "        # The possible attribute you can provide in the \"attr_to_keep\" are:\n",
    "        # - \"set_line_status\"\n",
    "        # - \"change_line_status\"\n",
    "        # - \"set_bus\": corresponds to changing the topology using the \"set_bus\" (equivalent to the\n",
    "        #   \"one_sub_set\" keyword in the \"attr_to_keep\" of the :class:`MultiDiscreteActSpace`)\n",
    "        # - \"change_bus\": corresponds to changing the topology using the \"change_bus\" (equivalent to the\n",
    "        #   \"one_sub_change\" keyword in the \"attr_to_keep\" of the :class:`MultiDiscreteActSpace`)\n",
    "        # - \"redispatch\"\n",
    "        # - \"set_storage\"\n",
    "        # - \"curtail\"\n",
    "        # - \"curtail_mw\" (same effect as \"curtail\")\n",
    "\n",
    "        ## customize observation space\n",
    "        ob_space = self.env_gym.observation_space\n",
    "        ob_space = ob_space.keep_only_attr([\"rho\"])\n",
    "        \n",
    "        self.env_gym.observation_space = ob_space\n",
    "\n",
    "        # 4. specific to rllib\n",
    "        self.action_space = self.env_gym.action_space\n",
    "        self.observation_space = self.env_gym.observation_space\n",
    "        self.step_count = 0\n",
    "        self.case_no = 0\n",
    "        self.reward_sum = 0\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env_gym.reset()\n",
    "        self.case_no += 1\n",
    "        self.reward_sum = 0\n",
    "        return obs\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        obs, reward, done, info = self.env_gym.step(action)\n",
    "        self.reward_sum += reward\n",
    "        return obs, reward, done, info\n",
    "CHECKPOINT_ROOT = \"tmp/rllib\"\n",
    "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
    "\n",
    "ray_results = os.getenv(\"HOME\") + \"/ray_results/\"\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check this link for RLlib Training API: https://docs.ray.io/en/master/rllib-training.html\n",
    "nb_step_train = 1\n",
    "\n",
    "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} saved {}\"\n",
    "for n in range(nb_step_train):  # remember: don't forge to change this number to perform an actual training !\n",
    "    from ray.rllib.agents import dqn  # import the type of agents (Change accordingly for PPO / ARS / APPO / A3C / A2C)\n",
    "    # fist initialize ray\n",
    "    config = dqn.DEFAULT_CONFIG.copy()\n",
    "    config[\"timesteps_per_iteration\"] = 10\n",
    "    ray.init()\n",
    "    try:\n",
    "        # then define a \"trainer\" (Change accordingly for PPO / ARS / APPO / A3C / A2C)\n",
    "        trainer = dqn.DQNTrainer(env=MyEnv, config={\n",
    "            \"env_config\": {\"env_name\":\"l2rpn_icaps_2021_small\"},  # config to pass to env class\n",
    "        })\n",
    "        # and then train it for a given number of iteration\n",
    "        for step in range(nb_step_train):\n",
    "            result = trainer.train()\n",
    "            \n",
    "            file_name = trainer.save(CHECKPOINT_ROOT)\n",
    "\n",
    "            print(s.format(\n",
    "              n + 1,\n",
    "              result[\"episode_reward_min\"],\n",
    "              result[\"episode_reward_mean\"],\n",
    "              result[\"episode_reward_max\"],\n",
    "              result[\"episode_len_mean\"],\n",
    "              file_name\n",
    "            ))\n",
    "            #print(pretty_print(result))\n",
    "    finally:   \n",
    "        # shutdown ray\n",
    "        ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "07_L2RPN_ICAPS test and install_baselines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_grid2op_alert",
   "language": "python",
   "name": "venv_grid2op_alert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
